\name{learn.task-class}
\alias{learn.task-class}
\alias{learn.task}
\title{learn.task}
\description{A learning task is the general description object for a machine learning experiment, which contains 
all initial setup for a learning task. It mainly includes the type of the learning task (e.g. lda), 
a dataframe and a formula. As this is just an abstract base class, 
you should not instantiate it directly but rather inherit from it in the learn.task classes of
your specific classifiers.}
\section{Slots}{\describe{\item{\code{wrapped.learner}:}{(\code{\link{wrapped.learner}}) Object of class \code{\linkS4class{wrapped.learner}}.}\item{\code{data}:}{(\code{\link{data.frame}}) Dataframe which includes all the data for the task.}\item{\code{weights}:}{(\code{\link{numeric}}) An optional vector of weights to be used in the fitting process. Default is a weight of 1 for every case.}\item{\code{formula}:}{(\code{\link{formula}}) A symbolic description of the model to be fitted.}\item{\code{data.desc}:}{(\code{\link{data.desc}}) Contains logical values describing properties of the dataframe e.g. whether it has 
characters or missing values (see desc and \code{\linkS4class{data.desc}}).}}}
\section{Methods}{\describe{\item{\code{\link[=benchmark,learn.task,resample.instance,resample.desc,list,list,logical-method]{benchmark}}}{\code{signature(learn.task = "learn.task", outer.resampling = "resample.instance", inner.resampling = "resample.desc", ranges = "list", measure = "list", all.tune.results = "logical")}: \code{benchmark} conducts a benchmark experiment for a single classifier on a single
data set. This consists of an inner stage and outer stage. At the outer stage a 
tuning set and a test set are repeatedly formed from the data through resampling 
(usually cross-validation or bootstrapping). The respective hyperparameters of the 
classifier are tuned on the tuning set again through an inner resampling process,
the classifier is trained on the complete tuning set with the best found 
hyperparameters and the performance is measured on the test set.}\item{\code{\link[=resample.fit,learn.task,resample.instance,list,character,logical,character-method]{resample.fit}}}{\code{signature(learn.task = "learn.task", resample.instance = "resample.instance", parset = "list", vars = "character", models = "logical", type = "character")}: Given the training and test indices (e.g. generated by cross-validation and generally specified by 
the \code{\linkS4class{resample.instance}} object) \code{resample.fit} 
fits the selected learner using the training sets and performs predictions for the test sets. These 
predictions are returned - encapsulated in a \code{\link{resample.result}} object. 
Optionally the fitted models are also stored.}\item{\code{\link[=resample.performance,learn.task,resample.instance,resample.result,list-method]{resample.performance}}}{\code{signature(learn.task = "learn.task", resample.instance = "resample.instance", resample.result = "resample.result", measure = "list")}: Measures the quality of predictions w.r.t. some loss function for a resampled fit.}\item{\code{\link[=initialize,learn.task-method]{initialize}}}{\code{signature( = "learn.task")}: Constructor.}\item{\code{\link[=[,learn.task-method]{[}}}{\code{signature( = "learn.task")}: Getter.}\item{\code{\link[=print,learn.task-method]{print}}}{\code{signature( = "learn.task")}: Prints the object by calling as.character.}\item{\code{\link[=show,learn.task-method]{show}}}{\code{signature( = "learn.task")}: Shows the object by calling as.character.}\item{\code{\link[=set.train.par,learn.task-method]{set.train.par}}}{\code{signature( = "learn.task")}: Set a parameter for the underlying train function of a wrapped learner. 
This is not meant for hyperparamters, pass these through the usual parset argument, but rather to
fix (somewhat techical) arguments which stay the same for the whole experiment. You should not have to use this too often.}\item{\code{\link[=set.predict.par,learn.task-method]{set.predict.par}}}{\code{signature( = "learn.task")}: Set a parameter for the underlying predict function of a wrapped learner. 
Used to fix (somewhat techical) arguments which stay the same for the whole experiment. Y
You should not have to use this too often.}}}

