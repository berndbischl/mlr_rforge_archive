<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
	<title>Resampling strategies</title>
	<link rel="stylesheet" type="text/css" href="formats.css">
	<style type="text/css">
	</style>
</head>

<body>
<h1>Resampling strategies</h1>

<p>Resampling strategies concern the process of generating new data from your data set D under examination 
in order to generate various training and test sets, which the learning method can be fitted and validated on. 
Here it is assumed that every resampling strategy consists of a couple of iterations, where for each one 
there are indices into D, defining the respective training and test sets. These iterations are implemented by 
storing the index set in a so called <term>resample.instance</term> object. The reasons for having the user 
create this data explicitly and not just set an option in a R function to choose the resampling method are:</p>
</p>
<ul>
	<li> It's easier to create paired experiments, where you train and test different methods on exactly 
the same sets, especially when you want to add another method to a comparism experiment you already did.
	<li> It's easy to add other resampling methods later on. You can simply use S4 inheritance, derive 
from the resample.instance class, but you don't have to touch any methods that use the resampling strategy.  
</ul>

<h2>Included strategies</h2>
<p>The packages come with a couple of predefined strategies</p>

<p><u>Subsampling</u></p>

<p>In each iteration i the data set D is randomly partitioned into a training and a test set according to a 
given percentage (maybe 2/3 training, 1/3 test set). If there is just one iteration, the strategy is commonly 
called <term>holdout</term> or <term>test sample estimation</term>.</p>
<pre>
	<com># split is the training set percentage</com>
	rin <- make.subsample.instance(iters=10, size=nrow(iris), split=2/3)

	<com># holdout</com>
	rin <- make.subsample.instance(iters=1, size=nrow(iris), split=2/3)
</pre>



<P><u>k-fold Crossvalidation</u></P>
<p>The data set is partitioned in k subparts of (nearly) equal size. In the i.th step of the k iterations, 
the i.th subpart is used as a test set, while the remainig parts form the training set.</p>
<pre>
	rin <- make.cv.instance(iters=10, size=nrow(iris))
</pre>

<P><u>Bootstrapping</u></P>
<p>B new data sets D<sub>1</sub>,..,D<sub>B</sub> are drawn from D with replacement, each of the same size as D. 
In the i.th iteration D<sub>i</sub> forms the training set, while the remaining element from D forms the test set.</p>
<pre>
	rin <- make.bs.instance(iters=10, size=nrow(iris))
</pre>



<h2>Further details</h2>
<p>
For every resampling strategy there is a description class inheriting from resample.desc (which completely 
characterizes the necessary parameters) and a class inheriting from resample.instance. This latter class 
takes the description object and takes care of the random drawing of indices. While this seems overly 
complicated, it is necessary as sometimes one only wants to describe the drawing process, while in other 
instances one wants to create the concrete index sets. Also, there are convenience methods, to make the 
construction process as easy as possible. Here's an example for crossvalidation:
</p>
<pre>
	<com># create a description for 10-fold CV </com>
	desc <- new("cv.desc", iters=10)
	<com># create the resample.instance, which defines the train/test indices</com>
	rin <- new("cv.instance", desc=desc, size=nrow(iris))
	<com># get the cv.instance directly</com>
	rin <- make.cv.instance(iters=10, size=nrow(iris))
</pre>


<P>
Asking the desc or resample.instance object for further information is easy, just use [ ] as the generic getter operator:

<pre>
	<com># decription object</com>
	<com># number of iters</com>
	desc["iters"]
	desc["iters"]
	
	<com># resample.instance object</com>
	<com># number of iters</com>
	rin["iters"]
	rin["iters"]
	<com># train/test indices for 3rd iteration</com>
	rin["train.inds", 3]
	rin["test.inds", 3]
	<com># train/test indices for 1st and 3rd iteration</com>
	rin["train.inds", c(1,3)]
	rin["test.inds", c(1,3)]
</pre>
<P>
Please refer to the help pages of the specific classes for a complete list of getters.
<P>
If you want to validate your classification method, using a certain resampling strategy, simply call resample.fit. <br>
For the example code, we use the standard iris data set and compare with cross-validation a 
Decision Tree and the Linear Discriminant Analysis:
<pre>
	<com># Classification task</com>
	ct <- make.classif.task(data=iris, formula=Species~.)

	<com># Resample instance for Cross-validation</com>
	rin <- make.res.instance("cv", ct, iters=3)

	<com># Merge learner, i.e. Decision Tree, classification task ct and resample instance rin</com>
	f1 <- resample.fit("rpart.classif", ct, rin)
	<com># Let's set a couple of hyperparamters for rpart</com>
	f1 <- resample.fit("rpart.classif", ct, rin, parset=list(minsplit=10, cp=0.03))	
	<com># Second resample.fit for LDA as learner </com>
	f2 <- resample.fit("lda", ct, rin)	

	<com># Let's see how the well both classifiers did</com>
	resample.performance(ct, f1)
	<res>
	$aggr1
	[1] 0.07333333

	$spread
	[1] 0.01154701
	
	$aggr2
	[1] 0.08 0.06 0.08

	$vals
	$vals[[1]]
	[1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1
	[39] 0 0 0 0 0 0 0 0 0 0 0 0

	$vals[[2]]
	[1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	[39] 0 1 0 0 0 1 0 0 0 0 0 0

	$vals[[3]]
	[1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
	[39] 0 0 0 1 0 0 0 1 1 0 0 0

	</res>
	resample.performance(ct, f2)
	<res>
	$aggr1
	[1] 0.02

	$spread
	[1] 0

	$aggr2
	[1] 0.02 0.02 0.02

	$vals
	$vals[[1]]
	[1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
	[39] 0 0 0 0 0 0 0 0 0 0 0 0

	$vals[[2]]
	[1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	[39] 0 0 0 0 0 0 0 0 0 0 0 0
	
	$vals[[3]]
	[1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	[39] 0 0 0 0 0 0 0 1 0 0 0 0
	</res>
</pre>

<!--
<h2>How to implement another resampling strategy</h2>
<P>
If you would like to use another resampling process, you have two options: If you think something useful is missing 
from the package, please email me! Otherwise you can easily extend it yourself. <P>
Lets assume we want to implement the following (nonsensical) strategy, called "foo": In the i.th iteration draw "bar" 
times i elements from D with replacement for the training set, the remaining elemnents form the test set.

   
<pre>
	# first the resample description

	# we inherit from resample.desc
	setClass("foo.desc", contains="resample.desc")
	
	# constructor
	setMethod(f = "initialize",	signature = "foo.desc",
		def = function(.Object, bar, iters) {
			.Object@bar <- bar
			# call super-constructor
			callNextMethod(.Object, instance.class="foo.instance" name="stupid foo strategy", iters=iters)
		}
	)

	# now the resample instance, the actual drawing

	# we inherit from resample.instance
	setClass("foo.instance", contains="resample.instance")                                                     

	# constructor
	setMethod(f = "initialize",	signature = "subsample.instance",
		def = function(.Object, desc, size) {
			# draw the training indices
			inds <- lapply(1:desc["iters"], function(i) sample(1:size, i*desc["bar"]))
			# super methods takes care of the rest
			callNextMethod(.Object, desc=desc, size=size, inds=inds)
		}
	)
	
	# convenience construction method
	make.foo.instance <- function(size, bar, iters) {
		desc <- new("foo.desc", iters=iters, bar=bar)
		return(new("foo.instance", desc=desc, size=size))
	}
)
</pre>
-->



</body>
</html>
