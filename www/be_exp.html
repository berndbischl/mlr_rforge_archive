<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
	<title>Benchmark experiments</title>
	<link rel="stylesheet" type="text/css" href="formats.css">
	<style type="text/css">
	</style>
</head>

<body>
<h1>Benchmark experiments</h1>

<P>
In order to get an unbiased estimate of the performance on new data, it is generally not enough to 
simply use repeated cross-validations for a given set of hyperparameters and methods 
(see <a href="tuning.html">tuning</a>), as this might produce an overly optimistic result. 
</P>
<P>
A better (although more time-consuming) approach is nesting two resampling methods. 
To make the explanation easier, let's take cross-validations, in this case also called "double cross-validation". 
In the so called "outer" cross-validation the data is split repeatedly into a (larger) training set and a (smaller) 
test set in the usual way. Now, in every outer iteration the learner is tuned on the training set by performing an 
"inner" cross-validation. The best found hyperparameters are selected , with these the learner is fitted to the 
complete "outer" training set and the resulting model is used to access the (outer) test set. 
This results in much more reliable estimates of true performance distribution of the learner for unseen data. 
These can now be used to estimate locations (e.g. of the mean or median performance value) and to compare 
learning methods in a fair way.
</P>

<P>In the following we will see four examples to show different benchmark settings:

<ol>
	<li>One data set	+	two classification algorithms</li>
  	<li>One data set	+	one classification algorithm	+	tuning</li>
  	<li>Three data sets	+	three classification algorithms	+	tuning</li>
  	<li>One data set	+	two classification algorithms	+	variable selection</li>
</ol>
</P>


<P>In general, benchmark experiments are constructed as the following graphic shows: </P>
<p><img src="pics/benchmark processing.png" width="55%"  border="1" alt=""></p>

<h3>Example 1 (One task, two learners, no tuning)</h3>

<pre>
	<com># Classification task with iris data set </com>
	ct <- make.task(data = iris, target = "Species")

	<com># Two learners to be compared </com>
	learners <- c("classif.lda", "classif.qda")

	<com># Define cross-validation indices </com>
	res <- make.res.desc("cv", iters = 5)

	<com># Merge it to a benchmark experiment </com>
	result <- bench.exp(learners, ct, res)
	<res>
	          classif.lda classif.qda
	mean.mmce  0.02000000  0.02666667
	sd.mmce    0.02981424  0.02788867 
	</res>
</pre>

<P>
The above code should be mainly self-explanatory. In the result every column corresponds to one learner.
The entries show the mean test error and its standard deviation for the final fitted model.</P>

<P>
But the Benchmark result contains much more information, which you can access if you want to see details. 
Let's have a look to the benchmark result from the example above:
</P>

<h3>Example 1 (continuation)</h3>

<pre>
	<com># Access further information </com>
	<com># The single performances of the cross-validation runs </com>
	result["perf"]
	<res>
	  classif.lda classif.qda
	1  0.06666667  0.06666667
	2  0.03333333  0.03333333
	3  0.00000000  0.00000000
	4  0.00000000  0.03333333
	5  0.00000000  0.00000000
	</res>

	<com># Confusion matrices - one for each learner</com>
	result["conf.mats"]
	<res>
	$iris
	$iris$classif.lda
	            predicted
	true         setosa versicolor virginica -SUM-
	  setosa         50          0         0     0
	  versicolor      0         48         2     2
	  virginica       0          1        49     1
	  -SUM-           0          1         2     3
	
	$iris$classif.qda
	            predicted
	true         setosa versicolor virginica -SUM-
	  setosa         50          0         0     0
	  versicolor      0         47         3     3
	  virginica       0          1        49     1
	  -SUM-           0          1         3     4
	</res>
</pre>


	
<h3>Example 2 (One task, one learner, tuning)</h3>

<P>
Now we have a learner with hyperparameters and we want to find out, which are the best ones ("tuning"). 
In that case we have two resampling levels (see <a href="resampling.html">Resampling</a>) <br>
We show an example with outer bootstrap and inner cross-validation, our learner will be k-nearest-neighbor.
</P>

<pre>
	<com># Classification task with iris data set </com>
	ct <- make.task(data = iris, target = "Species")

	<com># Range of hyperparameter k </com> 
	r <- list(k = 1:5)

	<com># Define "inner" cross-validation indices</com>
	inner.res <- make.res.desc("cv", iters = 3)   

	<com># Tune k-nearest-neighbor</com>
	knn.tuner <- make.tune.wrapper("classif.kknn", method = "grid", 
						       resampling = inner.res, 
    						       control = grid.control(ranges = r))

	<com># Define "outer" bootstrap indices </com>
	res <- make.res.desc("bs", iters = 5)

	<com># Merge it to a benchmark experiment </com>
	<com># Choose accuracy instead of default measure mean misclassification error</com>
	result <- bench.exp(knn.tuner, ct, res, measure = "acc")
	<res>
	  mean.acc     sd.acc 
	0.95591124 0.02875549  
	</res>
	
	<com># Which performances did we get in the single runs? </com>
	result["perf"]
	<res>
	        1         2         3         4         5 
	0.9814815 0.9807692 0.9387755 0.9148936 0.9636364 
	</res>

	<com># Which parameter belong to the performances? </com>
	result["tuned.par"] <b><font color="red">---include when in nice format----</font></b>
	<res>
	...
	</res>	

	<com># What does the confusion matrix look like? </com>
	result["conf.mats"]
	<res>
	$iris
	$iris$classif.kknn
	            predicted
	true         setosa versicolor virginica -SUM-
	  setosa         90          0         0     0
	  versicolor      0         73         6     6
	  virginica       0          5        83     5
	  -SUM-           0          5         6    11
	</res>
</pre>

<P>Of course everything works the same way if we exchange the resampling strategy either in the outer or inner run. 
They can be freely mixed.</P> 



<h3>Example 3 (Three tasks, three learners, tuning)</h3>
<P> Extensive example which shows a benchmark experiment with three data sets, three learner and tuning. </P>

<pre>
	library(dprep); library(mlbench)
	data(BreastCancer); data(Vehicle)
	<com># Classification task with three data sets </com>
	ct1 <- make.task("Iris", data = iris, target = "Species")
	ct2 <- make.task("Vehicle", data = Vehicle, target = "Class")
	ct3 <- make.task("BreastCancer", data = na.omit(BreastCancer), target = "Class", excluded = "Id")
	
	<com># Merge to one task</com>
	tasks = list(ct1 , ct2 , ct3)
	
	<com># Very small grid for SVM hyperparameters  </com>
	r <- list(C = 2^seq(-1,1), sigma = 2^seq(-1,1))
	
	<com># Define "inner" cross-validation indices</com>
	inner.res <- make.res.desc("cv", iters = 3)   
	
	<com># Tune a SVM</com>
	svm.tuner <- make.tune.wrapper("classif.ksvm", method = "grid", resampling = inner.res, 
					control = grid.control(ranges = r))
	
	<com># Three learners to be compared </com>
	learners <- c("classif.lda", "classif.rpart", svm.tuner)
	
	<com># Define "outer" cross-validation indices </com>
	res <- make.res.desc("cv", iters = 5)
	
	<com># Merge it to a benchmark experiment </com>
	result <- bench.exp(learners, tasks, res)
	<res>
	$Iris
	          classif.lda classif.rpart classif.ksvm
	mean.mmce  0.02000000    0.07333333   0.04000000
	sd.mmce    0.02981424    0.04346135   0.03651484
	
	$Vehicle
	          classif.lda classif.rpart classif.ksvm
	mean.mmce  0.21391577    0.31554473   0.27295510
	sd.mmce    0.03082199    0.03227883   0.04817659
	
	$BreastCancer
	          classif.lda classif.rpart classif.ksvm
	mean.mmce  0.03804208    0.06440532   0.05124517
	sd.mmce    0.02270102    0.01297826   0.02833322
	</res>	
	<com># Access detailed information about the result</com>
	<com># The single performances of the outer cross-validation for all learners and all tasks</com>
	result["perf"]
	<res>
	$Iris
	  classif.lda classif.rpart classif.ksvm
	1  0.00000000    0.13333333   0.10000000
	2  0.00000000    0.10000000   0.00000000
	3  0.00000000    0.00000000   0.03333333
	4  0.03333333    0.10000000   0.03333333
	5  0.06666667    0.06666667   0.10000000
	
	$Vehicle
	 classif.lda classif.rpart classif.ksvm
	1   0.2647059     0.4000000    0.2941176
	2   0.1893491     0.2781065    0.2426036
	3   0.2544379     0.3313609    0.2366864
	4   0.2248521     0.3136095    0.3136095
	5   0.1952663     0.3313609    0.3372781
	
	$BreastCancer
	  classif.lda classif.rpart classif.ksvm
	1  0.05109489    0.07299270   0.05839416
	2  0.03649635    0.04379562   0.04379562
	3  0.02919708    0.05109489   0.05839416
	4  0.06617647    0.05882353   0.04411765
	5  0.02205882    0.05882353   0.02941176
	</res>
	
	<com># Only for one task</com>
	result["perf", task = "Iris"]
	
	<com># Only for one learner</com>
	result["perf", learner = "classif.lda"]
		
	<com># Tuned parameter for SVM</com>
	result["tuned.par", learner = "classif.ksvm"]
	
	<com># Confusion matrix for one learner and one task</com>
	result["conf.mats", learner = "classif.rpart", task = "BreastCancer"]
	<res>
	           predicted
	 true        benign malignant -SUM-
	  benign       425        19    19
	  malignant     20       219    20
	  -SUM-         20        19    39
	</res>
	
	<com># Optimal performance of the inner (!) resampling, i.e. here 3-fold cross-validation</com>
	result["opt.perf", learner = "classif.ksvm"]
</pre>


<h3>Example 4 (One task, two learners, variable selection)</h3>
<P> Let's see how we can do variable selection in an benchmark experiment </P>

<pre>
	<com># Classification task with iris data set </com>
	ct <- make.task("iris", data=iris, target = "Species")  
	
	<com># Control object for variable selection</com>
	ctrl <- seq.control(beta = 100)
	
	<com># Inner resampling</com>
	inner <- make.res.desc("cv", iter=2)
	
	<com># Variable selection with Sequential Forward Search</com>
	vs <- make.varsel.wrapper("classif.lda", resampling = inner, method = "sfs", control = ctrl)
	
	<com># Let's compare two learners</com>
	learners <- c("classif.rpart", vs)
	
	<com># Define outer resampling </com>
	res <- make.res.desc("subsample", iter = 3)
	
	<com># Merge to a benchmark experiment</com>
	be <- bench.exp(tasks = ct, learners = learners, resampling = res)
	<res>
		          classif.rpart classif.lda
	mean.mmce          0.06  0.05333333
	sd.mmce            0.02  0.02309401 
	</res>
	
	<com># Which variables have been selected (in the outer resampling steps)?</com>
	be["sel.var", learner="classif.lda"]
	<res>
	[[1]]
	[1] "Petal.Length"
	
	[[2]]
	[1] "Petal.Width"
	
	[[3]]
	[1] "Petal.Width"  "Petal.Length" "Sepal.Length"
	</res>
</pre>


</body>
</html>
