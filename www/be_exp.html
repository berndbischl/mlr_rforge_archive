<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
	<title>Benchmark experiments</title>
	<link rel="stylesheet" type="text/css" href="formats.css">
	<style type="text/css">
	</style>
</head>

<body>
<h1>Benchmark experiments</h1>

<P>
In order to get an unbiased estimate of the performance on new data, it is generally not enough to 
simply use repeated cross-validations for a given set of hyperparameters and methods 
(see <a href="tuning.html">tuning</a>), as this might produce an overly optimistic result. 
<P>
A better (although more time-consuming) approach is nesting two resampling methods. 
To make the explanation easier, let's take cross-validations, in this case also called "double cross-validation". 
In the so called "outer" cross-validation the data is split repeatedly into a (larger) training set and a (smaller) 
test set in the usual way. Now, in every outer iteration the learner is tuned on the training set by performing an 
"inner" cross-validation. The best found hyperparameters are selected , with these the learner is fitted to the 
complete "outer" training set and the resulting model is used to access the (outer) test set. 
This results in much more reliable estimates of true performance distribution of the learner for unseen data. 
These can now be used to estimate locations (e.g. of the mean or median performance value) and to compare 
learning methods in a fair way.
<P>

<p><img src="pics/benchmark processing.png" width="55%"  border="1" alt=""></p>



Using mlr, setting up such an experiment becomes very easy: 

<h3>Example 1</h3>

<pre>
	<com># Classification task with iris data set </com>
	ct <- make.task(data = iris, target = "Species")

	<com># Very small grid for SVM hyperparameters </com> 
	r <- list(C = 2^seq(-1,1), sigma = 2^seq(-1,1))

	<com># Define "inner" cross-validation indices</com>
	inner.res <- make.res.desc("cv", iters = 3)   

	<com># Tune a SVM</com>
	svm.tuner <- make.tune.wrapper("classif.ksvm", method = "grid", 
							      resampling = inner.res, 
							      control = grid.control(ranges = r))

	<com># Three learner to be compared </com>
	learners <- c("classif.lda", "classif.qda", svm.tuner)

	<com># Define "outer" cross-validation indices </com>
	res <- make.res.desc("cv", iters = 5)

	<com># Merge it to a benchmark experiment </com>
	result <- bench.exp(learners, ct, res)
	<res>
	$iris
	          classif.lda classif.qda classif.ksvm
	mean.mmce  0.01666667  0.01666667   0.03333333
	sd.mmce    0.01924501  0.01924501   0.02721655
	</res>
</pre>



<P>
The above code should be mainly self-explanatory. In the result every column corresponds to one learner.
The entries show the mean test error and its standard deviation for the final fitted model.</P>

<P>
But the Benchmark result contains much more information, which you can access if you want to see details. 
Let's have a look to the benchmark result from the example above:
</P>

<h3>Example 1 (continuation)</h3>

<pre>
	<com># Access further information </com>
	<com># The single performances of the outer cross-validation </com>
	result["perf"]
	<res>
	$iris
	, , mmce
	
	  classif.lda classif.qda classif.ksvm
	1  0.00000000  0.00000000   0.06666667
	2  0.03333333  0.03333333   0.03333333
	3  0.03333333  0.03333333   0.03333333
	4  0.00000000  0.00000000   0.00000000
	5  0.03333333  0.03333333   0.06666667
	</res>
	<com># A list of the tuned parameters with tune- and test-performance </com>
	result["tuned.pars"] <b><font color="red">---does not work----</font></b>
	<res>
...
	</res>
	<com># Confusion matrices - one for each learner</com>
	result["conf.mats"] <b><font color="red">---does not work----</font></b>
	<res>
...
	</res>
</pre>


<P>
Of course everything works the same way if you exchange the resampling strategy either in the outer or inner run. 
They can be freely mixed. <br>
We show an example with outer bootstrap and inner cross-validation, our learner will be k-nearest-neighbor.
	
<h3>Example 2</h3>

<pre>
	<com># Classification task with iris data set </com>
	ct <- make.task(data = iris, target = "Species")

	<com># Range of hyperparameter k </com> 
	r <- list(k = 1:5)

	<com># Define "inner" cross-validation indices</com>
	inner.res <- make.res.desc("cv", iters = 3)   

	<com># Tune a SVM</com>
	knn.tuner <- make.tune.wrapper("classif.kknn", method = "grid", 
						       resampling = inner.res, 
    						       control = grid.control(ranges = r))

	<com># Define "outer" bootstrap indices </com>
	res <- make.res.desc("bs", iters = 5)

	<com># Merge it to a benchmark experiment </com>
	result <- bench.exp(knn.tuner, ct, res)
	<res>
	$iris
	          classif.kknn
	mean.mmce   0.05790844
	sd.mmce     0.04610950
	</res>
	
	<com># Which performances did we get in the single runs? </com>
	result["perf"]
	<res>
	$iris
	, , mmce
	
	  classif.kknn
	1   0.03571429
	2   0.04918033
	3   0.02173913
	4   0.12500000
	5   0.13207547
	</res>

	<com># Which parameter belong to the performances? </com>
	result["tuned.pars"] <b><font color="red">---does not work----</font></b>
	<res>
...
	</res>	

	<com># What does the confusion matrix look like? </com>
	result["conf.mats"] <b><font color="red">---does not work----</font></b>
	<res>
...
	</res>
</pre>


<P> When you want to add another learner to your existing benchmark experiment, this works easily in mlr. 
The big advantage is, that the same resample pairing is used as for the other learners. <br>
Let's take Example 2 and add another learner - NaiveBayes.

<h3>Example 2 (continuation)</h3>

<pre>
	new.result <- bench.add(learner = "classif.naiveBayes", task = ct, result = result)
	<res>
	<b><font color="red">---include----</font></b>
	</res>
</pre>


</body>
</html>
