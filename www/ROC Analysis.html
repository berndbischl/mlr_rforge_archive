<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
	<title>ROC Analysis</title>
	<link rel="stylesheet" type="text/css" href="formats.css">
	<style type="text/css">
	</style>
</head>

<body>
<h1>ROC Analysis</h1>


<p>The receiver operating characteristic (ROC) Curve is a graphical plot for a binary classifier and mainly used 
in signal detection theory. It compares the false positive rate (fall-out) on the horizontal axis 
to the true positive rate (sensitivity) on the vertical axis by varying the threshold. Sometimes costs of 
missclassification are unknown. So, in contrast to performance measures, where you can calculate only one 
performance value, a ROC Curve for one classifier shows an area of performance values in dependencies to 
different false positve and true positive rates. Another positive aspect for his graphical plot is the fact, that 
it can compare the performance of an imbalanced data set with a modified version. The following functions are 
created by including the features of the ROCR-Package.</p>
</p>

<p><img src="pics/ROC_Bsp.png" width="50%"  border="1" alt=""></p>

<h2>Benchmark Experiment</h2>
<pre>

<com># Loading the pima indian diabetes data set:
data(Pima.tr)</com>


<com># Classification task:</com>
ct <- make.task(data =Pima.tr, target = "type",positive="Yes")


<com># Choose your lerner, e.g. lda:</com>
learner <-  make.learner("classif.lda", predict.type="prob", id="lda")


<com># Make a 10-fold stratified cross-validation:</com>
res <- make.res.desc("stratcv",iters=10)


<com># Four measures:</com>
ms <- c("mmce","tpr","fpr","auc")


<com># Benchmark Experiment with predictions:</com>
be <- bench.exp(tasks=ct,learners=learner,resampling=res,measures=ms,predictions=T)


<com># Show different types of measures:</com>
be
<res>
# mean.mmce  0.23551378
# mean.tpr   0.55714286
# mean.fpr   0.12857143
# mean.auc   0.83511251
# sd.mmce    0.10324046
# sd.tpr     0.22897891
# sd.fpr     0.08867950
# sd.auc     0.09578202</res>
</pre>

You can also choose between other measures for calculating the performance of the ROC Curve. Now, we 
can create the plot of the ROC Curve seperatively for one classifier:


<h2>Calculating the ROC curve</h2>
<pre>
<com># Extract the predictions of the Benchmark Experiment:</com>
p <- be["prediction",task="Pima.tr"]


<com># Convert predictions that ROCR can interprets them:<com>
p.new <- as.ROCR.preds(p)


<com># Display the Performance for different values of the x-axis and y-axis:<com>
perf <- ROCR.performance(p.new,"tpr","fpr")


<com># Average all ROC Curves of the 10-fold stratified cross-validation and generate the resulting plot:</com>
ROCR.plot.performance(perf, avg="threshold")
</pre>

</body>
</html>

