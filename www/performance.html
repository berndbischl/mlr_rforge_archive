<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
	<title>Evaluation of Predictions</title>
	<link rel="stylesheet" type="text/css" href="formats.css">
	<style type="text/css">
	</style>
</head>

<body>

<h1>Evaluation of Predictions</h1>

<P>
The quality of predictions in mlr can be assessed w.r.t some performance measure or - more seldom - w.r.t. some 
loss function, which makes individual losses for all observations accessible. 
</P>
<P>
Typical performance measures are mean misclassification error (MMCE), accuracy or the ones based on ROC analysis
for classification and mean of squared errors (MSE) or absolute errors for regression. Those measures
aggregate the individual losses of predicted and true values to one number.
It's also possible to access the time to train a model, the time to compute the prediction and their sum as performance measures.
Available loss functions are the squared difference, the absolute one or the zero-one loss. 
</P>
<P>
In contrast to that, loss functions do not merge information, but compute transformations for every loss, i.e.
the difference between the prediction and its corresponding true label/value. 
Possible loss functions are the squared difference, the absolute one (for regression)or the zero-one loss (for classification). 
</P>
<P>
To see which performance measures and losses are implemented, have a look at <a href="rdocs/measures.html">?measures</a> 
or <a href="rdocs/losses.html">?losses</a> in R.
</P>

<h3>Classification example</h3>

<pre>

	<com># Classification task with iris data set </com>
	ct <- make.task(data = iris, target = "Species")

	<com># Linear Discriminant Analysis on every second observation </com>
	model <- train("classif.lda", task = ct, subset = seq(1,150,2))

	<com># Prediction on other half of the data set </com>
	preds <- predict(model, task = ct, subset = seq(2,150,2))
	
	<com># Compare predicted and true label with default performance measure mean misclassification error (MMCE)</com> 
	performance(preds)
	<res>
	$measures
	mmce 
	0.04 
	</res>
	
	<com># Let's have a look at some more performance measures and the zero-one losses</com>
	performance(preds, measures=list("mmce", "acc", "time.train", "time.predict", "time"), losses = "zero-one")
	
	<res>
	$measures
    	mmce          acc   time.train time.predict         time 
    	0.04         0.96         0.63         0.01         0.64 
	
	$losses
	    id zero-one
	1    2        0
	2    4        0
	3    6        0
	...
	65 130        1
	66 132        0
	67 134        1
	...
	73 146        0
	74 148        0
	75 150        0
	</res>
</pre>


<h3>Regression example</h3>


<p> Very analogous to above example </p>

<pre>
	<com># Regression task with BostonHousing data set </com>
	library(mlbench); data(BostonHousing)
	rt <- make.task(data = BostonHousing, target = "medv")

	<com># Training and test set indices </com>
	train.set <- seq(from = 1, to = 506, by = 2)
	test.set <- seq(from = 2, to = 506, by = 2)

	<com># Gradient Boosting Machine on training set </com>
	model <- train("regr.gbm", rt, subset = train.set, parset = list(n.trees=1000)) 

	<com># Prediction on test set data </com>
	preds <- predict(model, newdata = BostonHousing[test.set,])
	<res>
	[1] 22.395020 36.117794 25.031917 16.369151 17.798636 20.611659 22.606421
	[8] 22.671347 20.158904 20.559315 20.347968 15.692052 16.319054 16.349748
	...
	[253] 23.055107
	</res>

	<com># Compare predicted and true label with default measure MSE</com> 
	performance(preds)
	<res>
	$measures
	     mse 
	42.93442 
	</res>
	
</pre>
   

<h2>Evaluation of Predictions after Resampling</h2>

<P>
When you use some resampling strategy (for details see <a href="resampling.html">Resampling</a>), 
mlr offers the following possibilities to evaluate your predictions: 
Every single resampling iteration is handled as described in the explanation above, 
i.e. you train a model on a training part of the data set, predict on test set and compare 
predicted and true label w.r.t. some performance measure. This is proceeded for every
iteration so that you have e.g. ten performance value in the case of 10-fold cross-validation. 
The question arises, how to aggregate those values. You can specify that explicitly, the default is the mean.
Let's have a look at an example
</P>

<h3>Classification example</h3>

<pre>
	<com># Classification task</com>
	ct <- make.task(data = iris, target = "Species")

	<com># Resample instance for cross-validation</com>
	rin <- make.res.instance("cv", ct, iters = 3)

	<com># Learn Decision Tree</com>
	fit <- resample.fit("classif.rpart", ct, rin)

	<com># Let's see how well the classifier did w.r.t mean misclassification error and accuracy</com>
	performance(f1, measures = list("mmce", "acc"))
	<res>
	$measures
	  mmce  acc
	1 0.04 0.96
	2 0.06 0.94
	3 0.10 0.90
	
	$aggr
	           mmce        acc
	mean 0.06666667 0.93333333
	sd   0.03055050 0.03055050
	</res>

</pre>

<P>
As you can see above, in every fold of the 3-fold cross-validation you get one mean misclassification 
error (0.04, 0.06, 0.1) and after all three runs you aggregate them by mean to one value (0.0666).
Having a look at the single losses is of course possible as well.
</P>

</body>
</html>
