<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
	<title>Evaluation of Predictions</title>
	<link rel="stylesheet" type="text/css" href="formats.css">
	<style type="text/css">
	</style>
</head>

<body>

<h1>Evaluation of Predictions</h1>

<P>
The quality of predictions is usually measured w.r.t some performance measure or - more seldom - w.r.t. some 
loss function. 

Typical performance measures are mean misclassification error (MMCE), accuracy or true positive rate
for classification and sum of squared errors (SSE) or absolute deviations for regression. Those measures
merge the comparison of predicted and true label/value to one number.

To see which performance measures are implemented, have a look at <a href="rdocs/measures"> measures</a> 
in the R documentation.

In contrast to that, loss functions do not merge information, but compute transformations for every loss, i.e.
the difference between the prediction and its corresponding true label/value.
Possible loss functions are the squared difference, the absolute one or the zero-one loss, see 
<a href="rdocs/measures"> losses </a> in the R documentation.
</P>


<P>
A good starting point for classification are measures connected to ROCR curves and nice 
plots from the package <a target="_top" href="http://cran.r-project.org/web/packages/ROCR/index.html">ROCR</a>. 
I will probably integrate this more tightly at some point in the future.
</P>


<h3>Classification example</h3>

<pre>

	<com># Classification task with iris data set </com>
	ct <- make.task(data = iris, target = "Species")

	<com># Linear Discriminant Analysis on every second observation </com>
	model <- train("classif.lda", task = ct, subset = seq(1,150,2))

	<com># Prediction on other half of the data set </com>
	preds <- predict(model, task = ct, subset = seq(2,150,2))
	
	<com># Compare predicted and true label with default performance measure mean misclassification error (MMCE)</com> 
	performance(preds)
	<res>
	$measures
	mmce 
	0.04 
	</res>
	
	<com># Let's have a look at some more performance measures and the zero-one losses</com>
	performance(preds, measures = list("mmce", "acc"), losses = "zero-one")
	<res>
	$measures
	mmce  acc 
	0.04 0.96 
	
	$losses
	    id zero-one
	1    2        0
	2    4        0
	3    6        0
	...
	65 130        1
	66 132        0
	67 134        1
	...
	73 146        0
	74 148        0
	75 150        0
	</res>
</pre>


<h3>Regression example</h3>


<p> Very analogous to above example </p>

<pre>
	<com># Regression task with BostonHousing data set </com>
	library(mlbench); data(BostonHousing)
	rt <- make.task(data = BostonHousing, target = "medv")

	<com># Training and test set indices </com>
	train.set <- seq(from = 1, to = 506, by = 2)
	test.set <- seq(from = 2, to = 506, by = 2)

	<com># Gradient Boosting Machine on training set </com>
	model <- train("regr.gbm", rt, subset = train.set, parset = list(n.trees=1000)) 

	<com># Prediction on test set data </com>
	preds <- predict(model, newdata = BostonHousing[test.set,])
	<res>
	[1] 22.395020 36.117794 25.031917 16.369151 17.798636 20.611659 22.606421
	[8] 22.671347 20.158904 20.559315 20.347968 15.692052 16.319054 16.349748
	...
	[253] 23.055107
	</res>

	<com># Compare predicted and true label with default measure MSE</com> 
	performance(preds)
	<res>
	$measures
	     mse 
	42.93442 
	</res>
	
</pre>
   

<h2>Evaluation of Predictions after Resampling</h2>

<P>
When you use some resampling strategy (for details see <a href="resampling.html">Resampling</a>), 
mlr offers the following possibilities to evaluate your predictions: 
Every single resampling iteration is handled as described in the explanation above, 
i.e. you train a model on a training part of the data set, predict on test set and compare 
predicted and true label w.r.t. some performance measure. This is proceeded for every
iteration so that you have e.g. ten performance value in the case of 10-fold cross-validation. 
The question arises, how to aggregate those values. You can specify that explicitly, the default is the mean.
Let's have a look at an example
</P>

<h3>Classification example</h3>

<pre>
	<com># Classification task</com>
	ct <- make.task(data = iris, target = "Species")

	<com># Resample instance for cross-validation</com>
	rin <- make.res.instance("cv", ct, iters = 3)

	<com># Learn Decision Tree</com>
	fit <- resample.fit("classif.rpart", ct, rin)

	<com># Let's see how well the classifier did w.r.t mean misclassification error and accuracy</com>
	performance(f1, measures = list("mmce", "acc"))
	<res>
	$measures
	  mmce  acc
	1 0.04 0.96
	2 0.06 0.94
	3 0.10 0.90
	
	$aggr
	           mmce        acc
	mean 0.06666667 0.93333333
	sd   0.03055050 0.03055050
	</res>

</pre>

<P>
As you can see above, in every fold of the 3-fold cross-validation you get one mean misclassification 
error (0.04, 0.06, 0.1) and after all three runs you aggregate them by mean to one value (0.0666).
Having a look at the single losses is of course possible as well.
</P>

</body>
</html>
