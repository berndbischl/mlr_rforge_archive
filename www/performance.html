<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
	<title>Evaluation of Predictions</title>
	<link rel="stylesheet" type="text/css" href="formats.css">
	<style type="text/css">
	</style>
</head>

<body>

<h1>Evaluation of Predictions</h1>

<P>
The quality of predictions in mlr can be assessed w.r.t some performance measure or - more seldom - w.r.t. some 
loss function, which makes individual losses for all observations accessible. 
</P>
<P>
Typical performance measures are mean misclassification error (MMCE), accuracy or the ones based on ROC analysis
for classification and mean of squared errors (MSE) or absolute errors for regression. Those measures
aggregate the individual losses of predicted and true values to one number.
It is also possible to access the time to <a href="rdocs/train.html">"train"</a> a model, the time to compute the <a href="rdocs/predict.html">prediction</a> and their sum as performance measures.
</P>

<P>
In contrast to that, loss functions do not merge information, but compute transformations for every loss, i.e.
the difference between the prediction and its corresponding true label/value. 
Possible loss functions are the squared difference, the absolute one (for regression) or the zero-one loss (for classification). 
</P>
<P>
To see which performance measures and losses are implemented, have a look at <a href="rdocs/measures.html">?measures</a> 
or <a href="rdocs/losses.html">?losses</a>??? in R.
</P>

<h3>Classification example</h3>

<pre>

	<com># Classification task with iris data set </com>
	ct <- makeClassifTask(data = iris, target = "Species")

	<com># Linear Discriminant Analysis on every second observation </com>
	wm <- train("classif.lda", task = ct, subset = seq(1,150,2))

	<com># Prediction on other half of the data set </com>
	p <- predict(wm, task = ct, subset = seq(2,150,2))
	
	<com># Compare predicted and true label with performance measure mean misclassification error (MMCE)</com> 
	performance(p, measure = mmce)
	<res>
	[1] 0.04
	</res>
	
	<com># Let's have a look at some more performance measures and the zero-one losses</com>
	sapply(c(mmce=mmce, acc=acc, time.fit=time.fit, time.predict=time.predict, time.both=time.both), 
		function(m) performance(p, measure = m))
	###performance(p, measure=list("mmce", "acc", "time.train", "time.predict", "time"), losses = "zero-one")???
	# findet time.both nicht	
	<res>
	</res>
</pre>


<h3>Regression example</h3>


<p> Very analogous to above example </p>

<pre>
	<com># Regression task with BostonHousing data set </com>
	library(mlbench); data(BostonHousing)
	rt <- makeRegrTask(data = BostonHousing, target = "medv")

	<com># Training and test set indices </com>
	train.set <- seq(from = 1, to = 506, by = 2)
	test.set <- seq(from = 2, to = 506, by = 2)

	<com># Gradient Boosting Machine on training set </com>
	wl <- makeLearner("regr.gbm", n.trees = 1000)
	wm <- train("regr.gbm", rt, subset = train.set) 

	<com># Prediction on test set data </com>
	p <- predict(wm, newdata = BostonHousing[test.set,])
	<res>
	Prediction
	'data.frame':	253 obs. of  2 variables:
 	$ truth   : num  21.6 33.4 28.7 27.1 18.9 18.9 20.4 19.9 17.5 18.2 ...
 	$ response: num  22.3 23.3 22.4 22.1 22.1 ...
	</res>

	<com># Compare predicted and true label with measure MSE</com> 
	performance(p, measure = mse)
	<res>
	[1] 76.34979
	</res>
	
</pre>
   

<h2>Evaluation of Predictions after Resampling</h2>

<P>
When you use some resampling strategy (for details see <a href="resampling.html">Resampling</a>), 
mlr offers the following possibilities to evaluate your predictions: 
Every single resampling iteration is handled as described in the explanation above, 
i.e. you train a model on a training part of the data set, predict on test set and compare 
predicted and true label w.r.t. some performance measure. This is proceeded for every
iteration so that you have e.g. ten performance values in the case of 10-fold cross-validation. 
The question arises, how to aggregate those values. You can specify that explicitly, the default is the mean.
Let's have a look at an example
</P>

<h3>Classification example</h3>

<pre>
	<com># Classification task</com>
	ct <- makeClassifTask(data = iris, target = "Species")

	<com># Resample instance for cross-validation</com>
	res <- makeResampleDesc("cv", iters = 3)
	res <- make.res.instance(res, ct)

	<com># Learn Decision Tree</com>
	<com># Let's see how well the classifier did w.r.t mean misclassification error and accuracy</com>
	p <- resample("classif.rpart", task = ct, resampling = res, measures = list(mmce, acc))
	p
	<res>
	$measures.train
	  iter mmce acc
	1    1   NA  NA
	2    2   NA  NA
	3    3   NA  NA

	$measures.test
	  iter mmce  acc
	1    1 0.06 0.94
	2    2 0.08 0.92
	3    3 0.10 0.90

	$aggr
	mmce.test.mean   mmce.test.sd  acc.test.mean    acc.test.sd 
	          0.08           0.02           0.92           0.02 

	$pred
	Resampled Prediction for: cross-validation with 3 iterations.
	Predict: test
 

	$models
	NULL

	$extract
	$extract[[1]]
	NULL

	$extract[[2]]
	NULL

	$extract[[3]]
	NULL
	</res>

</pre>

<P>
As you can see above, in every fold of the 3-fold cross-validation you get one mean misclassification 
error (0.06, 0.08, 0.10) and after all three runs you aggregate them by mean to one value (0.08).
Having a look at the single losses is of course possible as well.???
</P>

</body>
</html>
