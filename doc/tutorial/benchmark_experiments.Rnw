Benchmark Experiments
=====================


In order to get an unbiased estimate of the performance on new data,
it is generally not enough to simply use repeated cross-validations
for a given set of hyperparameters and methods (see :doc:`tuning
</tutorial/tune>`), as this might produce an overly optimistic result.

A better (although more time-consuming) approach is nesting two
resampling methods.  To make the explanation easier, let's take
cross-validations, in this case also called "double cross-validation".
In the so called "outer" cross-validation the data is split repeatedly
into a (larger) training set and a (smaller) test set in the usual
way. Now, in every outer iteration the learner is tuned on the
training set by performing an "inner" cross-validation. The best found
hyperparameters are selected, with these the learner is fitted to the
complete "outer" training set and the resulting model is used to
access the (outer) test set.  This results in much more reliable
estimates of true performance distribution of the learner for unseen
data.  These can now be used to estimate locations (e.g. of the mean
or median performance value) and to compare learning methods in a fair
way.

In the following we will see four examples to show different benchmark settings:

# One data set	+	two classification algorithms</li>
# One data set	+	one classification algorithm	+	tuning
# Three data sets	+	three classification algorithms	+	tuning
# One data set	+	two classification algorithms	+	variable selection

In general, benchmark experiments are constructed as the following graphic shows: 

                  |benchmark_processing|

Example 1: One task, two learners, no tuning
----------------------------------------------

<<eval=FALSE>>=
library("mlr")

## Classification task with iris data set 
ct <- make.task(data = iris, target = "Species")

## Two learners to be compared 
learners <- c("classif.lda", "classif.qda")

## Define cross-validation indices 
res <- make.res.desc("cv", iters = 5)

result <- bench.exp(learners, ct, res)
@ 

The above code should be mainly self-explanatory. In the result every
column corresponds to one learner.  The entries show the mean test
error and its standard deviation for the final fitted model.</P>

But the Benchmark result contains much more information, which you can
access if you want to see details.  Let's have a look to the benchmark
result from the example above:

<<eval=FALSE>>=
## Access further information 
## The single performances of the cross-validation runs 
result["perf"]

## Confusion matrices - one for each learner
result["conf.mats"]
@ 
	
Example 2: One task, one learner, tuning
----------------------------------------

Now we have a learner with hyperparameters and we want to find out,
which are the best ones. In that case we have two resampling levels.

We show an example with outer bootstrap and inner cross-validation,
our learner will be k-nearest-neighbor.
<<eval=FALSE>>=
## Classification task with iris data set 
ct <- make.task(data = iris, target = "Species")

## Range of hyperparameter k  
r <- list(k = 1:5)

## Define "inner" cross-validation indices
inner.res <- make.res.desc("cv", iters = 3)   

## Tune k-nearest-neighbor
knn.tuner <- make.tune.wrapper("classif.kknn",
                               resampling = inner.res, 
                               control = grid.control(ranges = r))

## Define "outer" bootstrap indices 
res <- make.res.desc("bs", iters = 5)

## Merge it to a benchmark experiment 
## Choose accuracy instead of default measure mean misclassification error
result <- bench.exp(knn.tuner, ct, res, measure = "acc")
	
## Which performances did we get in the single runs? 
result["perf"]

## Which parameter belong to the performances? 
result["tuned.par"]

## What does the confusion matrix look like? 
result["conf.mats"]
@ 

Of course everything works the same way if we exchange the resampling
strategy either in the outer or inner run.  They can be freely
mixed.

Example 3: Three tasks, three learners, tuning
----------------------------------------------

Extensive example which shows a benchmark experiment with three data
sets, three learner and tuning.

<<eval=FALSE>>=
library("dprep")
library("mlbench")
data(BreastCancer)
data(Vehicle)

## Classification task with three data sets 
ct1 <- make.task("Iris", data = iris, target = "Species")
ct2 <- make.task("Vehicle", data = Vehicle, target = "Class")
ct3 <- make.task("BreastCancer", data = na.omit(BreastCancer), target = "Class", excluded = "Id")
	
## Merge to one task
tasks = list(ct1 , ct2 , ct3)
	
## Very small grid for SVM hyperparameters  
r <- list(C = 2^seq(-1,1), sigma = 2^seq(-1,1))

## Define "inner" cross-validation indices
inner.res <- make.res.desc("cv", iters = 3)   

## Tune a SVM
svm.tuner <- make.tune.wrapper("classif.ksvm", method = "grid", resampling = inner.res, 
                               control = grid.control(ranges = r))

## Three learners to be compared 
learners <- c("classif.lda", "classif.rpart", svm.tuner)

## Define "outer" cross-validation indices 
res <- make.res.desc("cv", iters = 5)

## Merge it to a benchmark experiment 
result <- bench.exp(learners, tasks, res)
	
## Only for one task
result["perf", task = "Iris"]

## Only for one learner
result["perf", learner = "classif.lda"]
		
## Tuned parameter for SVM
result["tuned.par", learner = "classif.ksvm"]

## Confusion matrix for one learner and one task
result["conf.mats", learner = "classif.rpart", task = "BreastCancer"]
	
## Optimal performance of the inner (!) resampling, i.e. here 3-fold cross-validation
result["opt.perf", learner = "classif.ksvm"]
@

Example 4: One task, two learners, variable selection
-----------------------------------------------------

Let's see how we can do :doc:`variable selection </tutorial/variable_selection>` in
an benchmark experiment:

<<eval=FALSE>>=
## Classification task with iris data set 
ct <- make.task("iris", data=iris, target = "Species")  
	
## Control object for variable selection
ctrl <- seq.control(beta = 100, method="sfs")

## Inner resampling
inner <- make.res.desc("cv", iter=2)

## Variable selection with Sequential Forward Search
vs <- make.varsel.wrapper("classif.lda", resampling = inner, control = ctrl)

## Let's compare two learners
learners <- c("classif.rpart", vs)

## Define outer resampling 
res <- make.res.desc("subsample", iter = 3)

## Merge to a benchmark experiment
be <- bench.exp(tasks = ct, learners = learners, resampling = res)
	
## Which variables have been selected (in the outer resampling steps)?
be["sel.var", learner="classif.lda"]
@

.. |benchmark_processing| image:: /_images/benchmark_processing.png
     :align: middle
     :width: 40em
     :alt: Benchmark processing pipeline