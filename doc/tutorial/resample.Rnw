<<echo=FALSE>>=
## Preload mlr for all coming blocks
library("mlr")

# Not strictly necessary, but otherwise we might get NAs later on 
## if 'rpart' is not installed.
library("rpart") 
@ 

Resampling
==========

In order to assess the performance of a learning algorithm usually resampling 
strategies are used. There are various methods how this can be done, e.g.
cross-validation and bootstrap just to mention two popular approaches. 
In **mlr** the resample_-function, depending on the chosen resampling strategy, 
fits the selected learner using the corresponding training sets and performs 
predictions for the corresponding training/test sets and calculates the chosen
performance measures.


Quick start
-----------

Classification example
......................


<<>>=
#LearnTask_, a learner (either a string or a Learner_ object) and 
#a resampling strategy, in this case 3-fold cross-validation
library("mlr")

task <- makeClassifTask(data=iris, target="Species")
rdesc <- makeResampleDesc("CV", iters=3)
r <- resample("classif.lda", task, rdesc)
r
@

In this example the peformance measure is mmce (mean misclassification error), 
the default for classification. See the documentation for measures_ for a 
complete list of performance measure available in **mlr**. More explanations 
concerning the evaluation of learning methods is given in Section Evaluating 
Learner Performance_.


Regression example
..................

For the regression example the BostonHousing data set is used again.

<<>>=
library("mlr")
library("mlbench")
data(BostonHousing)

task <- makeRegrTask(data = BostonHousing, target = "medv")
rdesc <- makeResampleDesc("CV", iters=3)
r <- resample("regr.lm", task, rdesc)
r
@

For regression the default performance measure is mse (mean squared error).




Further information
-------------------

Resampling strategies concern the process of sampling new data sets
from your data set :math:`D` under examination. One wants to generate
various training and test sets, which the learning method can be
fitted and validated on. Here it is assumed that every resampling
strategy consists of a couple of iterations, where for each one there
are indices into :math:`D`, defining the respective training and test
sets. 
These iterations are implemented by storing the index set in a
so called ResampleInstance_ object. The reasons for having the user
create this data explicitly and not just set an option in a R function
to choose the resampling method are:

* It is easier to create paired experiments, where you train and test
  different methods on exactly the same sets, especially when you want
  to add another method to a comparison experiment you already did.
* It is easy to add other resampling methods later on. You can
  simply use S4 inheritance, derive from the ResampleInstance_
  class, but you do not have to touch any methods that use the
  resampling strategy.

                     |resampling_figure|
                     
                     
Resample descriptions and resample instances
--------------------------------------------

two types of objects
a ResampleDesc_ object which stands for resample description and a ResampleInstance_
object. These can be generated by means of the factory methods makeResampleDesc_
and makeResampleInstance_.

For every resampling strategy there is a description class inheriting
from ResampleDesc_ (which completely characterizes the necessary
parameters) and a class inheriting from ResampleInstance_. This latter
class takes the description object and takes care of the random
drawing of indices. While this seems overly complicated, it is
necessary as sometimes one only wants to describe the drawing process,
while in other instances one wants to create the concrete index
sets. Also, there are convenience methods to make the construction
process as easy as possible.

<<eval=FALSE>>=
## get the cv.instance directly 
rinst  <- makeResampleInstance("CV", iters=10, size=nrow(iris))
@ 

Asking the ResampleDesc_ or ResampleInstance_ objects for further
information is easy, just inspect their slots: operator:

<<eval=FALSE>>=
## description object
## number of iterations
rdesc@iters
	
## resample.instance object
## number of iterations
rinst@iters

rinst["train.inds", 3]
rinst["test.inds", 3]

## train/test indices for first and third iteration
rinst["train.inds", c(1,3)]
rinst["test.inds", c(1,3)]
@ 

Please refer to the help pages of the specific classes for a complete
list of getters.


Included resampling strategies
------------------------------

The package comes with a couple of predefined strategies.

* 'CV' for cross-validation, 
* 'LOO' for leave-one-out, 
* 'StratCV' for stratified cross-validation, 
* 'RepCV' for repeated cross-validation,
* 'BS' for out-of-bag bootstrap, 
* 'Subsample' for subsampling,
* 'Holdout' for holdout.


Subsampling
...........

In each iteration :math:`i` the data set :math:`D` is randomly partitioned into a
training and a test set according to a given percentage (maybe 2/3
training, 1/3 test set). If there is just one iteration, the strategy
is commonly called `holdout` or `test sample estimation`.

<<eval=FALSE>>=
rdesc <- makeResampleDesc("Subsample", iters = 10, split = 2/3)
rdesc <- makeResampleDesc("Subsample", iters = 1, split = 2/3)
@ 

`k`-fold cross-validation
.........................

The data set is partitioned into :math:`k` subsets of (nearly) equal size. 
In the :math:`i`-th step of the :math:`k` iterations the :math:`i`-th subset is 
used as a test set, while the remaining parts form the training set.

<<eval=FALSE>>=
rdesc <- makeResampleDesc("CV", iters=10)
@ 

Bootstrapping
.............

:math:`B` new data sets :math:`D_1` to :math:`D_B` are drawn from
:math:`D` with replacement, each of the same size as :math:`D`. 
In the :math:`i`-th iteration :math:`D_i` forms the training set, 
while the remaining elements from :math:`D` not occuring in the training set 
form the test set.

<<eval=FALSE>>=
rdesc <- makeResampleDesc("BS", iters=10)
@ 


<p><img src="pics/resampling.[desc, instance].png" width="40%"  border="1" alt=""></p>


<p><img src="pics/Nested_Resampling.png" width="60%"  border="1" alt=""></p>



Evaluation of Predictions after Resampling
------------------------------------------

The resample_ function evaluates the performance of your learner using
a certain resampling strategy for a given machine learning task.

When you use some resampling strategy, **mlr** offers the following
possibilities to evaluate your predictions: Every single resampling
iteration is handled as described in the explanation above, i.e. you
train a model on a training part of the data set, predict on test set
and compare predicted and true label w.r.t. some performance
measure. This is proceeded for every iteration so that you have
e.g. ten performance values in the case of 10-fold cross-validation.
The question arises, how to aggregate those values. You can specify
that explicitly, the default is the mean.  Let's have a look at an
example 

object of class ResamplePrediction_ ...


Classification example
......................

For the example code, we use the standard iris data set and compare
with cross-validation a Decision Tree and the Linear Discriminant
Analysis:

Again, we create a classification task for the Iris data set and use
10-fold cross-validation to evaluate the learner:

<<>>=
## Classification task
task <- makeClassifTask(data=iris, target="Species")

## Resample instance for Cross-validation
rdesc <- makeResampleDesc("CV", iters=3)
rinst <- makeResampleInstance(rdesc, task=task)
@

Now we fit a decision tree for each fold and measure both the mean misclassification 
error (`mmce`) and the accuracy (`acc`):

<<>>=
## Merge learner, i.e. Decision Tree, classification task ct and resample instance rin
r1 <- resample("classif.rpart", task, rinst)
@

Let's set a couple of hyperparameters for rpart

<<>>=
lrn <- makeLearner("classif.rpart", minsplit=10, cp=0.03)
r1 <- resample(lrn, task, rinst, list(mmce, acc))	


## Second resample for LDA as learner 
r2 <- resample("classif.lda", task, rinst, list(mmce, acc))	

## Let's see how well both classifiers did w.r.t mean misclassification error and accuracy
r1
r2
@ 

If we want to see the individual values for each fold on the test set, we can access 
the `measures.test` element of the result list:

<<>>=
r1$measures.test
@ 

The aggregated performance values are stored in the `aggr` list element:

<<>>=
r1$aggr
@ 

As you can see above, in every fold of the 3-fold cross-validation you
get one mean misclassification error (0.06, 0.08, 0.10) and after all
three runs you aggregate them by mean to one value (0.08).  Having a
look at the single losses is of course possible as well.???

Regression example
......................

...

.. _resample: ../_static/rdocs/mlr/resample.html
.. _ResamplePrediction: ../_static/rdocs/mlr/ResamplePrediction-class.html
.. _LearnTask: ../_static/rdocs/mlr/LearnTask-class.html
.. _Learner: ../_static/mlr/Learner-class.html
.. _measures: ../_static/mlr/measures.html
.. _Performance performance.html
.. _ResampleDesc: ../_static/rdocs/mlr/ResampleDesc-class.html
.. _ResampleInstance: ../_static/rdocs/mlr/ResampleInstance-class.html
.. _makeResampleDesc: ../_static/rdocs/mlr/makeResampleDesc.html
.. _makeResampleInstance: ../_static/rdocs/mlr/makeResampleInstance.html


.. |resampling_figure| image:: /_images/resampling.png
     :align: middle
     :width: 40em
     :alt: Resampling illustration