<<echo=FALSE>>=
## Preload mlr for all coming blocks
library("mlr")

# Not strictly necessary, but otherwise we might get NAs later on 
## if 'rpart' is not installed.
library("rpart") 
@ 

Resampling Learners
===================

Resampling strategies concern the process of sampling new data sets
from your data set :math:`D` under examination. One wants to generate
various training and test sets, which the learning method can be
fitted and validated on. Here it is assumed that every resampling
strategy consists of a couple of iterations, where for each one there
are indices into :math:`D`, defining the respective training and test
sets. These iterations are implemented by storing the index set in a
so called ResampleInstance_ object. The reasons for having the user
create this data explicitly and not just set an option in a R function
to choose the resampling method are:

* It is easier to create paired experiments, where you train and test
  different methods on exactly the same sets, especially when you want
  to add another method to a comparison experiment you already did.
* It is easy to add other resampling methods later on. You can
  simply use S4 inheritance, derive from the ResampleInstance_
  class, but you do not have to touch any methods that use the
  resampling strategy.

                     |resampling_figure|

Included strategies
-------------------

The packages come with a couple of predefined strategies

Subsampling
...........

In each iteration i the data set D is randomly partitioned into a
training and a test set according to a given percentage (maybe 2/3
training, 1/3 test set). If there is just one iteration, the strategy
is commonly called <term>holdout</term> or <term>test sample
estimation</term>.

<<eval=FALSE>>=
rin <- makeResampleDesc("subsample", iters=10, split=2/3)
rin <- makeResampleDesc("subsample", iters=1, split=2/3)
@ 

`k`-fold cross-validation
.........................

The data set is partitioned in k subparts of (nearly) equal size. In the i.th step of the k iterations, 
the i.th subpart is used as a test set, while the remaining parts form the training set.</p>

<<>>=
rin <- makeResampleDesc("CV", iters=10)
@ 

Bootstrapping
.............

:math:`B` new data sets :math:`D_1` to :math:`D_B` are drawn from
:math:`D` with replacement, each of the same size as D. In the :math:`i`-th
iteration :math:`D_i` forms the training set, while the remaining elements
not occuring in the training set form the test set.

<<eval=FALSE>>=
rin <- makeResampleDesc("bs", iters=10)
@ 


<p><img src="pics/resampling.[desc, instance].png" width="40%"  border="1" alt=""></p>


<p><img src="pics/Nested_Resampling.png" width="60%"  border="1" alt=""></p>

Further details
---------------

For every resampling strategy there is a description class inheriting
from ResampleDesc_ (which completely characterizes the necessary
parameters) and a class inheriting from ResampleInstance_. This latter
class takes the description object and takes care of the random
drawing of indices. While this seems overly complicated, it is
necessary as sometimes one only wants to describe the drawing process,
while in other instances one wants to create the concrete index
sets. Also, there are convenience methods, to make the construction
process as easy as possible. Here is an example for cross-validation:

<<eval=FALSE>>=
## create a description for 10-fold cross-validation
desc <- new("cv.desc", iters=10) 

## create the ResampleInstance, which defines the train/test indices
rin <- new("cv.instance", desc=desc, size=nrow(iris))

## get the cv.instance directly 
rin  <- makeResampleInstance("CV", iters=10, size=nrow(iris))
@ 

Asking the ResampleDesc_ or ResampleInstance_ objects for further
information is easy, just inspect their slots: operator:

<<eval=FALSE>>=
## description object
## number of iterations
desc@iters
	
## resample.instance object
## number of iterations
rin@iters

rin["train.inds", 3]
rin["test.inds", 3]

## train/test indices for first and third iteration
rin["train.inds", c(1,3)]
rin["test.inds", c(1,3)]
@ 

Please refer to the help pages of the specific classes for a complete
list of getters.

The resample_ function evaluates the performance of your learner using
a certain resampling strategy for a given machine learning task.

For the example code, we use the standard iris data set and compare
with cross-validation a Decision Tree and the Linear Discriminant
Analysis:

<<>>=
## Classification task
ct <- makeClassifTask(data=iris, target="Species")

## Resample instance for Cross-validation
rin <- makeResampleDesc("CV", iters=3)
rin <- makeResampleInstance(rin, task=ct)

## Merge learner, i.e. Decision Tree, classification task ct and resample instance rin
f1 <- resample("classif.rpart", ct, rin)
## Let's set a couple of hyperparameters for rpart
wl <- makeLearner("classif.rpart", minsplit=10, cp=0.03)
f1 <- resample(wl, ct, rin, list(mmce, acc))	
## Second resample for LDA as learner 
f2 <- resample("classif.lda", ct, rin, list(mmce, acc))	

## Let's see how well both classifiers did w.r.t mean misclassification error and accuracy
f1
f2
@ 

.. _resample: _static/rdoc/mlr/resample.html
.. _ResampleDesc: _static/rdoc/mlr/ResampleDesc-class.html
.. _ResampleInstance: _static/rdoc/mlr/ResampleInstance-class.html

.. |resampling_figure| image:: /_images/resampling.png
     :align: middle
     :width: 40em
     :alt: Resampling illustration