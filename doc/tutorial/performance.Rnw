Evaluating Learner Performance
===============================

The quality of predictions in **mlr** can be assessed w.r.t. some
performance measure.

Typical performance measures are **mean misclassification error** (MMCE),
**accuracy or the ones** based on ROC analysis for classification and *mean
of squared errors* (MSE) or absolute errors for regression. 
It is also possible to access the time to train the model,
the time to compute the prediction and their sum as performance
measures.

To see which performance measures are implemented, have a
look at measures_.
In order to calculate the performance measures the
function performance_ is used.


Classification example
----------------------

We fit Linear Discriminant Analysis on a subset of the iris data set and calculate
the mean misclassification error (MMCE) on the test data set.

<<>>=
library("mlr")

task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.lda")
mod <- train(lrn, task = task, subset = seq(1,150,2))
pred <- predict(mod, task = task, subset = seq(2,150,2))

performance(pred, measure = mmce)
@ 
	
Let's have a look at some more performance measures. Note that in order to asses 
the time needed for training the fitted model has to be passed.

<<>>=
performance(pred, measure = acc)

performance(pred = pred, measure = timepredict)

performance(pred = pred, measure = timetrain, model = mod)
performance(pred = pred, measure = timeboth, model = mod)
@

A nice method to calculate multiple performace measures at once makes use of the popular R funktion sapply. We define a list of measures and apply the performance function for each element.

<<>>=
ms <- list("mmce" = mmce, "acc" = acc, "timetrain" = timetrain, "timeboth" = timeboth)
sapply(ms, function(the.ms) {
  performance(pred = pred, measure = the.ms, model = mod)
})
@

Binary classification
.....................

In the two-class case many more measures are available. In the following example
besides the accuracy the false positive and false negative rates are computed.

<<>>=
library("mlbench")
data(Sonar)

task <- makeClassifTask(data = Sonar, target = "Class", positive = "M")
lrn <- makeLearner("classif.rpart")
mod <- train(lrn, task = task)
pred <- predict(mod, task = task)

performance(pred, measure = acc)
performance(pred, measure = fpr)
performance(pred, measure = fnr)
@ 


Note that in order to calculate the AUC, the area under the ROC (receiver 
operating characteristic) curve, we have to make sure that posterior
probabilities are predicted, i.e. set the predict type of the Learner_ to "prob".

<<>>=
library("mlbench")
data(Sonar)

task <- makeClassifTask(data = Sonar, target = "Class", positive = "M")
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- train(lrn, task = task)
pred <- predict(mod, task = task)

performance(pred, measure = auc)
@ 

For more information on ROC analysis see Section :doc:`roc_analysis`.


Regression example
------------------

For regression everything works analogous to the above examples.
We again use the BostonHousing data set, fit a Gradient Boosting Machine on a
training set and calculate the mean of squared errors and the mean of absolute 
errors on the test data set.

<<>>=
library(mlbench)
data(BostonHousing)

task <- makeRegrTask(data = BostonHousing, target = "medv")

## Training and test set indices 
train.set <- seq(from = 1, to = nrow(BostonHousing), by = 2)
test.set <- seq(from = 2, to = nrow(BostonHousing), by = 2)

## Gradient Boosting Machine on training set 
lrn <- makeLearner("regr.gbm", n.trees = 1000)
mod <- train(lrn, task, subset = train.set) 

## Prediction on test set data 
pred <- predict(mod, newdata = BostonHousing[test.set,])

## Compare predicted and true label with measure MSE 
performance(pred, measure = mse)

performance(pred, measure = mae)
@

Construction additional performance measures
............................................

Maybe you want to evaluate a measure, which ist not yet implemented in **mlr**. The makeMeasure_ function is a simple way to construct your own performance measure. Let's implement our own version of the already mentioned MMCE. For this purpose we write a simple function, that computes the measure on the basis of the predictions and subsequently wrap it in a Measure object. Then we work with it as usual with the performance_ function. See the R documentation of the makeLearner_ function for details on the various parameters.

<<>>=
# Define the measure.
my.mmce <- function(task, model, pred, extra.args) {
  tb <- table(pred$data$response, pred$data$truth)
  1 - sum(diag(tb)) / sum(tb)
}

# encapsulate the function with a Measure object.
my.mmce <- makeMeasure(id = "my.mmce", minimize = TRUE, classif = TRUE, allowed.pred.types = "response", fun = my.mmce)

# Create classification task and learner
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.lda")
mod <- train(lrn, task)
pred <- predict(mod, newdata= iris)

# Compare predicted and true label with our measure.
performance(pred, measure = my.mmce)

# Apparently the result coincides with the mlr implementaion.
performance(pred, measure = mmce)
@

.. _Learner: ../_static/rdocs/mlr/Learner-class.html
.. _performance: ../_static/rdocs/mlr/performance.html
.. _measures: ../_static/rdocs/mlr/measures.html