ROC Analysis
============

The receiver operating characteristic (ROC) Curve is a graphical plot
for a binary classifier and mainly used in signal detection theory. It
compares the false positive rate (fall-out) on the horizontal axis to
the true positive rate (sensitivity) on the vertical axis by varying
the threshold. Sometimes costs of misclassification are unknown. So,
in contrast to performance measures, where you can calculate only one
performance value, a ROC Curve for one classifier shows an area of
performance values in dependencies to different false positive and true
positive rates. Another positive aspect for his graphical plot is the
fact, that it can compare the performance of an imbalanced data set
with a modified version. The following functions are created by
including the features of the ROCR_-Package.

  
                      |roc_example|
                      
Prediction
----------

as.ROCR.prediction_
ROCR.performance_
ROCR.plot.performance_

<<>>=
library("mlr")
data(Pima.tr)

task <- makeClassifTask(data = Pima.tr, target = "type", positive = "Yes")
lrn <- makeLearner("classif.lda", predict.type = "prob", id = "lda")

mod <- train(lrn, task)
pred <- predict(mod, task = task)

ROCRpred <- as.ROCR.prediction(pred)
ROCRperf <- ROCR.performance(ROCRpred, "tpr","fpr")
plot(ROCRperf)
@

Resample prediction
-------------------
<<eval=FALSE>>=
library("mlr")
data(Pima.tr)

task <- makeClassifTask(data = Pima.tr, target = "type", positive = "Yes")
lrn <- makeLearner("classif.lda", predict.type = "prob", id = "lda")
rdesc <- makeResampleDesc("CV", iters = 5)

r <- resample(lrn, task, rdesc)

ROCRpred <- as.ROCR.prediction(r$pred)

ROCRperf <- ROCR.performance(ROCRpred, "tpr","fpr")
plot(ROCRperf)


plot(ROCRperf, avg = "threshold")
@

                      
                      
                      
Benchmark experiment
--------------------

<<eval=FALSE>>=
library("mlrBenchmark")

## Loading the pima indian diabetes data set:
data(Pima.tr)

## Classification task:
task <- makeClassifTask(data = Pima.tr, target = "type", positive = "Yes")

## Choose your lerner, e.g. lda:
lrn <- makeLearner("classif.lda", predict.type = "prob", id = "lda")

## Make a 10-fold stratified cross-validation:
rdesc <- makeResampleDesc("stratcv", iters=10)

## Four measures:
measures <- c(mmce, tpr, fpr, auc)

## Benchmark Experiment with predictions:
res <- bench.exp(tasks=task,learners=lrn,resampling=rdesc,measures=measures,predictions=T)

## Show different types of measures:
print(res)
@ 

You can also choose between other measures for calculating the
performance of the ROC Curve. Now, we can create the plot of the ROC
Curve separately for one classifier:

<<eval=FALSE>>=
## Extract the predictions of the Benchmark Experiment:
p <- be["prediction",task="Pima.tr"]

## Convert predictions that ROCR can interprets them:#
p.new <- as.ROCR.preds(p)

## Display the Performance for different values of the x-axis and y-axis:#
perf <- ROCR.performance(p.new,"tpr","fpr")

## Average all ROC Curves of the 10-fold stratified cross-validation and generate the resulting plot:
ROCR.plot.performance(perf, avg="threshold")
@ 



.. _as.ROCR.prediction: ../_static/rdocs/mlr/as.ROCR.prediction.html
.. _ROCR.performance: ../_static/rdocs/mlr/ROCR.performance.html
.. _ROCR.plot.performance: ../_static/rdocs/mlr/ROCR.plot.performance.html
.. _ROCR: http://cran.r-project.org/web/packages/ROCR/index.html




.. |roc_example| image:: /_images/roc_example.png
     :align: middle
     :width: 40em
     :alt: Example ROC curve